{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6hqxepCc0E4"
   },
   "source": [
    "# A model for classifying comments as positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I52JYLh-c2Pe"
   },
   "source": [
    "## Contents:\n",
    "1. Reviewing of the data and preprocessing.\n",
    "2. Training models.\n",
    "3. Testing models.\n",
    "4. Overall conclusion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJ17G9RGc0E4"
   },
   "source": [
    "The online store \"Wikishop\" is launching a new service. Now users can edit and supplement product descriptions, just like in wiki communities. That is, customers can suggest their own edits and comment on the changes of others. The store needs a tool that will search for toxic comments and send them for moderation. \n",
    "\n",
    "We will train a model to classify comments as positive or negative. We have a dataset with labels about toxicity of edits. \n",
    "\n",
    "We will build a model with an F1 metric value of no less than 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ap5r3T9w6_g9"
   },
   "source": [
    "## Description of data:\n",
    "* text - it contains the text of the comment,\n",
    "* toxic - the target feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHieaVtHc0E5"
   },
   "source": [
    "## 1. Reviewing of the data and preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cENYyLxbEZN-"
   },
   "source": [
    "Import the libraries and tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-dbQceYc0E5",
    "outputId": "25915d3b-1969-41c6-8e50-b6a938e58ed9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /home/vladimir/anaconda3/lib/python3.9/site-packages (3.3.5)\n",
      "Requirement already satisfied: scipy in /home/vladimir/anaconda3/lib/python3.9/site-packages (from lightgbm) (1.10.0)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from lightgbm) (1.2.1)\n",
      "Requirement already satisfied: wheel in /home/vladimir/anaconda3/lib/python3.9/site-packages (from lightgbm) (0.38.4)\n",
      "Requirement already satisfied: numpy in /home/vladimir/anaconda3/lib/python3.9/site-packages (from lightgbm) (1.23.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.1)\n",
      "Requirement already satisfied: xgboost in /home/vladimir/anaconda3/lib/python3.9/site-packages (1.7.5)\n",
      "Requirement already satisfied: scipy in /home/vladimir/anaconda3/lib/python3.9/site-packages (from xgboost) (1.10.0)\n",
      "Requirement already satisfied: numpy in /home/vladimir/anaconda3/lib/python3.9/site-packages (from xgboost) (1.23.5)\n",
      "Requirement already satisfied: spacy in /home/vladimir/anaconda3/lib/python3.9/site-packages (3.5.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy) (22.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy) (1.10.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: setuptools in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from en-core-web-sm==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (22.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: setuptools in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (65.6.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/vladimir/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm\n",
    "!pip install xgboost\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LJKYuG2Xhafs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from tqdm import notebook\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGAoPvotduzh"
   },
   "source": [
    "Let's load additional data for NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1QUebacLdvNx",
    "outputId": "047ffc60-79a1-4f5d-9a02-e847a55338c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vladimir/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/vladimir/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/vladimir/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vladimir/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "moHCj2Ybn71y",
    "outputId": "39398246-5e4d-4313-ec7e-b4dadc5a81cb"
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNXwQSwxiTfk"
   },
   "source": [
    "Open the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uajc2DTzieBl"
   },
   "outputs": [],
   "source": [
    "url = 'https://code.s3.yandex.net/datasets/toxic_comments.csv'\n",
    "data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "3lMNV27ti7yU",
    "outputId": "3b3d866a-1e03-4fe1-bc83-cf5d00299ffe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fa0iz0KjjDsv"
   },
   "source": [
    "Let's look at the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yCwZ32xDjIel",
    "outputId": "2201e5e7-8ece-426b-e7ee-2bb9b5b136c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LVPu35KceUTH"
   },
   "outputs": [],
   "source": [
    "data = data[['text', 'toxic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mkSRBd_yjSnx",
    "outputId": "75e192c4-ece2-4152-cb80-86931068891c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYsOR7JBjV1z"
   },
   "source": [
    "There are no duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9j2kj2kG-qru"
   },
   "source": [
    "We'll change the data type to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "scPMGaW19O3L"
   },
   "outputs": [],
   "source": [
    "data['toxic'] = data['toxic'].astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BMy5K_DT96u4",
    "outputId": "be5a19f5-e30a-4314-f339-7585f154e48a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  uint8 \n",
      "dtypes: object(1), uint8(1)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSqYK7gbArT3"
   },
   "source": [
    "Let's look at the class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tC5YpSSvA1nZ",
    "outputId": "e31a4cac-863b-4d35-83c9-1116f797f1c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   0.90\n",
       "1   0.10\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['toxic'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nwtroUxA5v9"
   },
   "source": [
    "The target is not balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrEuljmlBO7d"
   },
   "source": [
    "### We'll prepare the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiWzGjv-EGah"
   },
   "source": [
    "We'll clean the text and lemmatize it with a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1IfIskZ-9rTR"
   },
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    l = WordNetLemmatizer()\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lemm_list = []\n",
    "    for word in word_list:\n",
    "        lemm_list.append(l.lemmatize(word))\n",
    "    lemm_text = \" \".join(lemm_list)\n",
    "    return lemm_text\n",
    "\n",
    "def clear_text(text):\n",
    "    re_text = re.sub(r\"[^a-zA-Z ]\", \" \", text)\n",
    "    re_text = re.sub(r\"[UTC]\", \"\", re_text)\n",
    "    re_text = re_text.lower()\n",
    "    return \" \".join(re_text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2dy5eEGHAR3"
   },
   "source": [
    "Let's clean the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "id": "WLm7r3P-HEmq",
    "outputId": "38c95437-6b74-4c1a-c8fc-54cb559e95cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.48 s, sys: 112 ms, total: 4.6 s\n",
      "Wall time: 4.73 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>clear_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>d aww he matches this background colour i m se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man i m really not trying to edit war it s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>more i can t make any real suggestions on impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  D'aww! He matches this background colour I'm s...      0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                                          clear_text  \n",
       "0  explanation why the edits made under my userna...  \n",
       "1  d aww he matches this background colour i m se...  \n",
       "2  hey man i m really not trying to edit war it s...  \n",
       "3  more i can t make any real suggestions on impr...  \n",
       "4  you sir are my hero any chance you remember wh...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data['clear_text'] = data['text'].apply(clear_text)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YL5FwbQ7j3Mp"
   },
   "source": [
    "We'll lemmatize it with spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "woUuxDC4ZqyO"
   },
   "outputs": [],
   "source": [
    "def lemmatize_spacy(text, lemmatizer):\n",
    "    doc = lemmatizer(text)\n",
    "    lemm_text = \" \".join([token.lemma_ for token in doc])\n",
    "    return lemm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BFnByPniZ3_a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27min 24s, sys: 2.76 s, total: 27min 27s\n",
      "Wall time: 27min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sp = spacy.load('en_core_web_sm', \n",
    "                disable=['parser', 'ner'])\n",
    "\n",
    "data['lemms'] = data['clear_text'].apply(lemmatize_spacy, lemmatizer=sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZs38x2KBmTI"
   },
   "source": [
    "We'll split the dataset into samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "IzdKTMKJBmpX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4627      I don t mean to intrude but I have notice the ...\n",
       "23542     god or whoever whatever I now decree you the c...\n",
       "128384    an we keep this neat and sequential we have al...\n",
       "31111     here s what you say may rama block expire jun ...\n",
       "41207     sir giggsy have often say that he like to keep...\n",
       "                                ...                        \n",
       "54715     indeed bigdunc that page rightly say that para...\n",
       "75564     I m not try to make a point or anything except...\n",
       "71196     forgive my cruddy formattingi m still relative...\n",
       "55751     alk grasshopper scout I move your comment from...\n",
       "11543                                   oppose wp ommonname\n",
       "Name: lemms, Length: 127433, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features, test_features, train_target, test_target = train_test_split(\n",
    "    data.drop('toxic', axis=1),\n",
    "    data['toxic'],\n",
    "    test_size=0.2,\n",
    "    random_state=12345,\n",
    "    stratify=data['toxic'],\n",
    "    )\n",
    "\n",
    "corpus_train = train_features['lemms']\n",
    "corpus_test = test_features['lemms']\n",
    "corpus_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJQcs68fda_R"
   },
   "source": [
    "We'll use TfidfVectorizer to clean the bag of words and add stop words to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "nmoaiKhydlwY"
   },
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "tf_idf_train = count_tf_idf.fit_transform(corpus_train)\n",
    "tf_idf_test = count_tf_idf.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2yLqfOnc0E6"
   },
   "source": [
    "## 2. Training models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQpX2A4IeuII"
   },
   "source": [
    "To automate the process, we'll write a function to select hyperparameters and calculate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WgvDIJNbfZ1b"
   },
   "outputs": [],
   "source": [
    "analisys = pd.DataFrame({'model':[], 'F1_model':[], 'F1_on_train':[]})\n",
    "all_models = []\n",
    "\n",
    "def train_model(model, parameters):\n",
    "    \n",
    "    model_random = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=parameters,\n",
    "        scoring='f1', \n",
    "        n_jobs=-1,\n",
    "        cv=4, \n",
    "        verbose=0\n",
    "        )\n",
    "    \n",
    "    start = time()\n",
    "    model_random.fit(tf_idf_train, train_target)\n",
    "    print('Time to search for hyperparameters %.2f sec.' %(time() - start))\n",
    "    \n",
    "    f1 = model_random.best_score_\n",
    "    f1_on_train = f1_score(train_target, model_random.predict(tf_idf_train))\n",
    "    \n",
    "    print('Best params:', model_random.best_params_)\n",
    "    print('F1 of trained model:', round(f1, 3))\n",
    "    print('F1 on train set:', round(f1_on_train, 3))\n",
    "\n",
    "    all_models.append(model_random)\n",
    "    row = []\n",
    "    row.extend([model, f1, f1_on_train])\n",
    "    analisys.loc[len(analisys.index)] = row\n",
    "    \n",
    "    return model_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6N6Dwqix3GO9"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "20PzkKQU3Wo7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladimir/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/vladimir/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/home/vladimir/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/home/vladimir/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/home/vladimir/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/vladimir/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/home/vladimir/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/vladimir/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/vladimir/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/vladimir/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to search for hyperparameters 94.23 sec.\n",
      "Best params: {'penalty': 'l2', 'class_weight': 'balanced'}\n",
      "F1 of trained model: 0.75\n",
      "F1 on train set: 0.832\n",
      "CPU times: user 19 s, sys: 1.39 s, total: 20.3 s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ran_lr = {\n",
    "    \"penalty\": ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    \"class_weight\": ['balanced', 'none'],\n",
    "    }\n",
    "\n",
    "logr = LogisticRegression(max_iter=100)\n",
    "\n",
    "lr_random = train_model(logr, ran_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lglTTI9939ul"
   },
   "source": [
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "FFrFz6VM3-JZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to search for hyperparameters 1480.32 sec.\n",
      "Best params: {'max_depth': 59}\n",
      "F1 of trained model: 0.701\n",
      "F1 on train set: 0.838\n",
      "CPU times: user 1min 6s, sys: 284 ms, total: 1min 6s\n",
      "Wall time: 24min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ran_grid_tree = {\n",
    "    \"max_depth\": list(range(50, 60)),\n",
    "    }\n",
    "\n",
    "dtr = DecisionTreeClassifier()\n",
    "\n",
    "dtr_random = train_model(dtr, ran_grid_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ck9qpyPI4NwH"
   },
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "c6roDgi04Quo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to search for hyperparameters 3250.55 sec.\n",
      "Best params: {'n_estimators': 16, 'max_depth': 314}\n",
      "F1 of trained model: 0.633\n",
      "F1 on train set: 0.914\n",
      "CPU times: user 5min 47s, sys: 3.34 s, total: 5min 51s\n",
      "Wall time: 54min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ran_grid_forest = {\n",
    "    'max_depth': list(range(300, 320)),\n",
    "    'n_estimators': [12, 16],\n",
    "    }\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "rfc_random = train_model(rfc, ran_grid_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLQbVO3XgLT9"
   },
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ABAnCOZhgNU9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to search for hyperparameters 5162.22 sec.\n",
      "Best params: {'max_depth': 25, 'learning_rate': 0.45}\n",
      "F1 of trained model: 0.77\n",
      "F1 on train set: 0.892\n",
      "CPU times: user 7min 11s, sys: 2.44 s, total: 7min 13s\n",
      "Wall time: 1h 26min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rand_lgbm_param = {\n",
    "    'max_depth': [20, 25, 30],\n",
    "    'learning_rate': [0.15, 0.3, 0.45],\n",
    "    }\n",
    "\n",
    "gbm = lgb.LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    n_jobs=-1,\n",
    "    )\n",
    "\n",
    "gbm_random = train_model(gbm, rand_lgbm_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWqPJJzsgQHr"
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "YRu_Eul3gQgC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to search for hyperparameters 4416.96 sec.\n",
      "Best params: {'max_depth': 9, 'learning_rate': 0.5}\n",
      "F1 of trained model: 0.753\n",
      "F1 on train set: 0.861\n",
      "CPU times: user 4min 42s, sys: 505 ms, total: 4min 42s\n",
      "Wall time: 1h 13min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rand_xgb_param = {\n",
    "    'max_depth': [6, 7, 8, 9],\n",
    "    'learning_rate': [0.3, 0.5, 1.0],\n",
    "    }\n",
    "\n",
    "xb = xgb.XGBClassifier(booster='gbtree', \n",
    "                      use_rmm=True,\n",
    "                      n_jobs=-1)\n",
    "\n",
    "xb_random = train_model(xb, rand_xgb_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQ2618HGgiqS"
   },
   "source": [
    "### Model analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "sXbhayqdgjOs"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>F1_model</th>\n",
       "      <th>F1_on_train</th>\n",
       "      <th>names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.83</td>\n",
       "      <td>LogisticRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.84</td>\n",
       "      <td>DecisionTree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier(n_jobs=-1)</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.91</td>\n",
       "      <td>RandomForest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LGBMClassifier()</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.89</td>\n",
       "      <td>LightGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBClassifier(base_score=None, booster='gbtree...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.86</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  F1_model  F1_on_train  \\\n",
       "0                               LogisticRegression()      0.75         0.83   \n",
       "1                           DecisionTreeClassifier()      0.70         0.84   \n",
       "2                  RandomForestClassifier(n_jobs=-1)      0.63         0.91   \n",
       "3                                   LGBMClassifier()      0.77         0.89   \n",
       "4  XGBClassifier(base_score=None, booster='gbtree...      0.75         0.86   \n",
       "\n",
       "                names  \n",
       "0  LogisticRegression  \n",
       "1        DecisionTree  \n",
       "2        RandomForest  \n",
       "3            LightGBM  \n",
       "4             XGBoost  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_names = pd.DataFrame({'names':['LogisticRegression', \n",
    "                                   'DecisionTree', \n",
    "                                   'RandomForest', \n",
    "                                   'LightGBM', \n",
    "                                   'XGBoost']})\n",
    "analisys = pd.concat([analisys, all_names], \n",
    "                     axis=1, \n",
    "                     join='inner')\n",
    "display(analisys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ufNg2WdgjxC"
   },
   "source": [
    "### Conclusion:\n",
    "We can recommend three models to the customer as the most accurate according to the given metric. Let's test the following models: LogisticRegression, XGBoost, and LightGBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErluypnwetzI"
   },
   "source": [
    "## 3. Testing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "jbj6k_KHeuug"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 LogisticRegression on the test set: 0.7579626394301842\n",
      "F1 XGBoost on the test set: 0.7513116474291709\n",
      "F1 LightGBM on the test set: 0.7751728790689829\n"
     ]
    }
   ],
   "source": [
    "predicted = lr_random.predict(tf_idf_test)\n",
    "print('F1 LogisticRegression on the test set:', f1_score(test_target, predicted))\n",
    "\n",
    "predicted = xb_random.predict(tf_idf_test)\n",
    "print('F1 XGBoost on the test set:', f1_score(test_target, predicted))\n",
    "\n",
    "predicted = gbm_random.predict(tf_idf_test)\n",
    "print('F1 LightGBM on the test set:', f1_score(test_target, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-RAj4X4c0E6"
   },
   "source": [
    "## 4. Overall conclusion.\n",
    "A dataset of 159292 rows was received for work. The data was prepared, the data type of the target feature was changed to save memory. The dataset was checked for missing values and duplicates, which were not found. Further data preparation, text cleaning and lemmatization were carried out before training the models. \n",
    "\n",
    "The dataset was divided into samples, and a function with hyperparameter selection was written to automate the calculation of metrics for the used models. As a result of testing the selected models, only LightGBM meets the customer's requirements, its F1 metric exceeds the acceptable threshold of 0.75 and is 0.776. This result was obtained based on the use of RandomizedSearchCV - a hyperparameter selection tool. \n",
    "\n",
    "When selecting hyperparameters using GridSearchCV, higher accuracy values can be achieved for the considered models, but unfortunately this will lead to an increase in the training time, which is already 33 minutes on average.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
